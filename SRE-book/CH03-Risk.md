# Chapter 3: Embracing Risk

> **Source**: [Google SRE Book - Chapter 3](https://sre.google/sre-book/embracing-risk/)

---

## The Core Insight

> **100% reliability is the wrong target.**

Why?
- Extreme reliability limits feature development speed
- Dramatically increases costs
- Users can't tell the difference (their phone/network is less reliable anyway)
- A user on a 99% reliable smartphone **cannot distinguish** 99.99% from 99.999% service reliability

**SRE Goal:** Balance unavailability risk with rapid innovation and efficient operations to optimize **overall user happiness**.

---

## Managing Risk

### The Cost of Reliability

Reliability improvements have **nonlinear costs**:
- Each incremental "nine" may cost **100x more** than the previous
- Costs come from redundancy, testing, and opportunity cost of slower development

### The SRE Approach

| Traditional Ops | SRE Approach |
|-----------------|--------------|
| Maximize uptime | Manage risk on a continuum |
| Reliability = good | Reliability = expensive trade-off |
| More is always better | "Reliable enough, but no more" |

> ğŸ’¡ **Key Principle:** Availability target is both a **minimum AND maximum**. Exceeding it wastes opportunities for features, tech debt reduction, or cost savings.

---

## Measuring Service Risk

### Two Ways to Measure Availability

**1. Time-based availability (traditional):**
```
Availability = Uptime / (Uptime + Downtime)
```

**2. Aggregate/Request-based availability (Google's approach):**
```
Availability = Successful Requests / Total Requests
```

> Google uses request-based because globally distributed services are usually "partially up" at all times.

**Example:**
- 2.5M requests/day with 99.99% target
- Can serve up to **250 errors** and still meet target

### Why Request-Based Works Better

- Works for serving systems AND batch/pipeline systems
- Measures from end-user perspective
- Applies to any system with "successful/unsuccessful units of work"

---

## The "Nines" Reference

| Availability | Downtime/Year | Downtime/Month |
|--------------|---------------|----------------|
| 99% (2 nines) | 3.65 days | 7.2 hours |
| 99.9% (3 nines) | 8.76 hours | 43.8 minutes |
| 99.99% (4 nines) | 52.56 minutes | 4.38 minutes |
| 99.999% (5 nines) | 5.26 minutes | 26.3 seconds |

---

## Risk Tolerance of Services

### Consumer Services

Work with product owners to answer:

| Factor | Questions to Ask |
|--------|------------------|
| **Target Availability** | What do users expect? Does it tie to revenue? Free or paid? What do competitors offer? Consumer or enterprise? |
| **Types of Failures** | Constant low-rate failures vs. occasional outage? Privacy implications? |
| **Cost** | ROI of another nine? Does revenue justify the cost? |
| **Other Metrics** | Latency requirements? Throughput needs? |

**Real Examples:**

| Service | Target | Reasoning |
|---------|--------|-----------|
| Google Apps for Work | 99.9% (with penalties) | Enterprise users depend on it for daily work |
| YouTube (2006) | Lower target | Rapid feature development was more important |
| Ads Frontend | Allowed maintenance windows | Business hours usage pattern |

### Infrastructure Services

**Key difference:** Multiple clients with varying needs.

**Example: Bigtable**
| Client Type | Priority | Queue Preference |
|-------------|----------|------------------|
| Low-latency (user-facing) | Response time | Almost always empty |
| Throughput (batch/analytics) | Volume processed | Never empty |

**Solution:** Offer **multiple service tiers**
- Low-latency clusters: More slack capacity, higher cost
- Throughput clusters: Run hot, less redundancy, 10-50% of cost

> ğŸ’¡ **Strategy:** Expose cost differences to clients. Let them choose the right trade-off.

---

## Error Budgets

### The Dev vs. SRE Tension

| Product Development | SRE |
|---------------------|-----|
| Evaluated on velocity | Evaluated on reliability |
| Push code quickly | Push back against change |
| "Ship it!" | "Is it safe?" |

**Common conflict points:**
- Testing thoroughness
- Release frequency
- Canary duration/size

> *"Hope is not a strategy."* â€” Google SRE unofficial motto

### How Error Budgets Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Product Management sets quarterly SLO              â”‚
â”‚       (e.g., 99.999% success rate)                     â”‚
â”‚                     â†“                                   â”‚
â”‚  2. Monitoring measures actual uptime                   â”‚
â”‚                     â†“                                   â”‚
â”‚  3. Error Budget = SLO - Actual                         â”‚
â”‚       (e.g., 0.001% allowed failure rate)              â”‚
â”‚                     â†“                                   â”‚
â”‚  4. Budget remaining â†’ releases continue                â”‚
â”‚     Budget exhausted â†’ releases halt                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**
- SLO: 99.999% queries succeed
- Error budget: 0.001% failure rate per quarter
- Incident causes 0.0002% failures â†’ spends **20% of quarterly budget**

### Benefits of Error Budgets

1. **Shared incentive** â€” both teams want the same thing
2. **Self-policing** â€” devs slow down when budget is low
3. **Objective decisions** â€” data replaces politics
4. **Highlights costs** â€” shows price of over-reliability
5. **Flexible response** â€” slow releases, don't just stop

### What Eats the Budget?

- Poor code quality
- Rushed releases
- **AND external factors** (network outages, datacenter failures)

> Everyone shares responsibility. External failures reduce release velocity too.

---

## Key Insights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EMBRACING RISK: SUMMARY                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Managing reliability = Managing risk (costly)       â”‚
â”‚                                                         â”‚
â”‚  2. 100% is never the right target                      â”‚
â”‚     - Impossible to achieve                             â”‚
â”‚     - More than users notice                            â”‚
â”‚     - Match risk to business needs                      â”‚
â”‚                                                         â”‚
â”‚  3. Error budgets align incentives                      â”‚
â”‚     - Joint ownership between SRE and Dev               â”‚
â”‚     - Defuses outage discussions                        â”‚
â”‚     - Teams reach same conclusions about risk           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Decision Framework

When setting availability targets, ask:

1. **What do users expect?** (Not what's technically possible)
2. **What's the revenue impact?** (Can you calculate $/nine?)
3. **What do competitors offer?** (Match or differentiate)
4. **What's the cost to improve?** (Is ROI positive?)
5. **What's the failure mode?** (Partial vs. total outage)
6. **Is there an error budget?** (Align dev and ops incentives)

---

*Written by Marc Alvidrez, Edited by Kavita Guliani*  
*Error Budgets section by Mark Roth, Edited by Carmela Quinito*  
*From "Site Reliability Engineering: How Google Runs Production Systems" (O'Reilly, 2016)*
